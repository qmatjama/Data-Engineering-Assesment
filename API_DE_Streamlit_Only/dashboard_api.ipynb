{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72163c30-f917-4171-a95f-76cccc7de6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "# XCAP token transfer API endpoint\n",
    "base_url = \"https://xcap-mainnet.explorer.xcap.network/api/v2/token-transfers\"\n",
    "\n",
    "# Set the export path\n",
    "excel_path = r\"C:\\Users\\user\\Desktop\\personal\\DE_Assesment\\DE_Assesment_Results.xlsx\"\n",
    "\n",
    "# Configs\n",
    "MAX_PAGES = 50  # You can increase or decrease this\n",
    "PAGE_DELAY = 0.3  # Delay between API calls in seconds\n",
    "\n",
    "\n",
    "#TASK 1: Data Extraction & Processing\n",
    "\n",
    "# Function to fetch token transfers\n",
    "def fetch_all_token_transfers():\n",
    "    page = 1\n",
    "    all_data = []\n",
    "\n",
    "    while page <= MAX_PAGES:\n",
    "        try:\n",
    "           # print(f\"🚀 Fetching page {page}...\")\n",
    "            response = requests.get(base_url, params={\"page\": page}, timeout=10)\n",
    "   #monitoring the igestion\n",
    "            if response.status_code != 200:\n",
    "                print(f\" Error {response.status_code} on page {page}\")\n",
    "                break\n",
    "\n",
    "            data = response.json()\n",
    "            items = data.get(\"items\", [])\n",
    "\n",
    "            if not items:\n",
    "                print(\" No more data — stopping.\")\n",
    "                break\n",
    "\n",
    "            all_data.extend(items)\n",
    "            page += 1\n",
    "            time.sleep(PAGE_DELAY)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\" Request failed on page {page}: {e}\")\n",
    "            break\n",
    "# report fetched data in the pipeline\n",
    "    print(f\" Done! Total records fetched: {len(all_data)}\")\n",
    "    return pd.json_normalize(all_data)\n",
    "\n",
    "# Run the function\n",
    "df = fetch_all_token_transfers()\n",
    "\n",
    "#df.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "fields_to_keep = [\n",
    "    'transaction_hash',\n",
    "    'token.symbol',\n",
    "    'total.value',\n",
    "    'from.hash',\n",
    "    'to.hash',\n",
    "    'timestamp',\n",
    "    'token.exchange_rate',\n",
    "    'type',\n",
    "    'token.decimals'\n",
    "]\n",
    "\n",
    "# Filter columns that exist\n",
    "available_fields = [col for col in fields_to_keep if col in df.columns]\n",
    "df = df[available_fields]\n",
    "\n",
    "# Export the results to xlsx \n",
    "df_fetched = df[available_fields].copy()  # Important to avoid SettingWithCopyWarning\n",
    "\n",
    "\n",
    "\n",
    "                          #STEP 2 TRANSFORM AND CLEAN DATA\n",
    "#dedup the transections\n",
    "df = df.drop_duplicates(subset=fields_to_keep)\n",
    "\n",
    "# Convert timestamp to datetime (ISO 8601 format)\n",
    "#df.loc[:, 'timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "\n",
    "# Clean and convert numeric fields\n",
    "df.loc[:, 'total.value'] = pd.to_numeric(df['total.value'], errors='coerce').fillna(0)\n",
    "df.loc[:, 'token.decimals'] = pd.to_numeric(df['token.decimals'], errors='coerce').fillna(18)\n",
    "\n",
    "# Convert token amount to human-readable format\n",
    "df.loc[:, 'normalized_value'] = df['total.value'] / (10 ** df['token.decimals'])\n",
    "\n",
    "# Clean exchange rate and calculate USD value\n",
    "df.loc[:, 'token.exchange_rate'] = pd.to_numeric(df['token.exchange_rate'], errors='coerce').fillna(0)\n",
    "df.loc[:, 'usd_value'] = df['normalized_value'] * df['token.exchange_rate']\n",
    "\n",
    "                                                # TASK 2: Compute Key Blockchain Metrics\n",
    "\n",
    "# Total Asset Supply = Minted - Burned \n",
    "minted_total = df[df['type'] == 'token_minting']['normalized_value'].sum()\n",
    "burned_total = df[df['type'] == 'token_burning']['normalized_value'].sum()\n",
    "total_supply = minted_total - burned_total\n",
    "\n",
    "# Number of Unique Tokens (by token.symbol)\n",
    "unique_tokens = df['token.symbol'].nunique()\n",
    "\n",
    "# Total Number of Transactions \n",
    "total_transactions = len(df)\n",
    "\n",
    "#  Breakdown by Type \n",
    "minted_tokens = minted_total\n",
    "burned_tokens = burned_total\n",
    "transferred_tokens = df[df['type'] == 'token_transfer']['normalized_value'].sum()\n",
    "\n",
    "# Total Transaction Volume in USD \n",
    "total_usd_volume = df['usd_value'].sum()\n",
    "\n",
    "#  Output all key metrics\n",
    "print(\"\\n Task 2: Key Metrics\")\n",
    "print(\"1. Total Asset Supply:\", total_supply)\n",
    "print(\"2. Unique Tokens:\", unique_tokens)\n",
    "print(\"3. Total Transactions:\", total_transactions)\n",
    "print(\"4. Tokens Minted:\", minted_tokens)\n",
    "print(\"5. Tokens Burned:\", burned_tokens)\n",
    "print(\"6. Tokens Transferred:\", transferred_tokens)\n",
    "print(\"7. Total Transaction Volume (USD):\", total_usd_volume)\n",
    "\n",
    "\n",
    "# Prepare results as a dictionary\n",
    "metrics = {\n",
    "    \"Metric\": [\n",
    "        \"1. Total Asset Supply\",\n",
    "        \"2. Unique Tokens\",\n",
    "        \"3. Total Transactions\",\n",
    "        \"4. Tokens Minted\",\n",
    "        \"5. Tokens Burned\",\n",
    "        \"6. Tokens Transferred\",\n",
    "        \"7. Total Transaction Volume (USD)\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_supply,\n",
    "        unique_tokens,\n",
    "        total_transactions,\n",
    "        minted_tokens,\n",
    "        burned_tokens,\n",
    "        transferred_tokens,\n",
    "        total_usd_volume\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame for display\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "\n",
    "# === Export both sheets ===\n",
    "#with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    # Export Task 2 metrics table\n",
    "  #  metrics_df.to_excel(writer, sheet_name=\"task2_metrics_table\", index=False)\n",
    "\n",
    "    # Export fetched records\n",
    "   # df_fetched.to_excel(writer, sheet_name=\"Total fetched records\", index=False)\n",
    "\n",
    "\n",
    "#df\n",
    "# TASK 3\n",
    "\n",
    "# Ensure we are working on a fresh copy to avoid future issues\n",
    "df_task3 = df.copy()\n",
    "\n",
    "# Convert normalized_value in case it's not already computed\n",
    "#df_task3['normalized_value'] = pd.to_numeric(df_task3['total.value'], errors='coerce') / (10 ** df_task3['token.decimals'])\n",
    "\n",
    "# Step 1: Tokens Sent per Address ===\n",
    "tokens_sent = df_task3[df_task3['type'] == 'token_transfer'].groupby('from.hash')['normalized_value'].sum().reset_index()\n",
    "tokens_sent.columns = ['address', 'tokens_sent']\n",
    "\n",
    "# === Step 2: Tokens Received per Address ===\n",
    "tokens_received = df_task3[df_task3['type'] == 'token_transfer'].groupby('to.hash')['normalized_value'].sum().reset_index()\n",
    "tokens_received.columns = ['address', 'tokens_received']\n",
    "\n",
    "# === Step 3: Tokens Minted per Address (to.hash) ===\n",
    "tokens_minted = df_task3[df_task3['type'] == 'token_minting'].groupby('to.hash')['normalized_value'].sum().reset_index()\n",
    "tokens_minted.columns = ['address', 'tokens_minted']\n",
    "\n",
    "#  Step 4: Tokens Burned per Address (from.hash) ===\n",
    "tokens_burned = df_task3[df_task3['type'] == 'token_burning'].groupby('from.hash')['normalized_value'].sum().reset_index()\n",
    "tokens_burned.columns = ['address', 'tokens_burned']\n",
    "\n",
    "# === Step 5: Merge All Together to Calculate Holdings ===\n",
    "from functools import reduce\n",
    "dfs = [tokens_minted, tokens_burned, tokens_received, tokens_sent]\n",
    "df_combined = reduce(lambda left, right: pd.merge(left, right, on='address', how='outer'), dfs).fillna(0)\n",
    "\n",
    "# === Step 6: Calculate Final Token Holdings ===\n",
    "df_combined['token_holding'] = (df_combined['tokens_minted'] - df_combined['tokens_burned']) + (df_combined['tokens_received'] - df_combined['tokens_sent'])\n",
    "\n",
    "# === Step 7: Top 10 by Holdings ===\n",
    "top10_holdings = df_combined.sort_values(by='token_holding', ascending=False).head(10)\n",
    "\n",
    "# === Step 8: Top 10 by Sent ===\n",
    "top10_sent = tokens_sent.sort_values(by='tokens_sent', ascending=False).head(10)\n",
    "\n",
    "# === Step 9: Top 10 by Received ===\n",
    "top10_received = tokens_received.sort_values(by='tokens_received', ascending=False).head(10)\n",
    "\n",
    "# === Print Results ===\n",
    "print(\"\\n Top 10 Addresses Holding the Most Tokens:\")\n",
    "print(top10_holdings[['address', 'token_holding']].to_string(index=False))\n",
    "\n",
    "print(\"\\n Top 10 Addresses by Tokens Sent:\")\n",
    "print(top10_sent.to_string(index=False))\n",
    "\n",
    "print(\"\\n Top 10 Addresses by Tokens Received:\")\n",
    "print(top10_received.to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Task 3 Summary Export Preparation ===\n",
    "\n",
    "# Recalculate token_holding (if not already done above)\n",
    "df_combined['token_holding'] = (\n",
    "    df_combined['tokens_minted']\n",
    "    - df_combined['tokens_burned']\n",
    "    + df_combined['tokens_received']\n",
    "    - df_combined['tokens_sent']\n",
    ")\n",
    "\n",
    "# Top 10s\n",
    "top10_holdings = df_combined.sort_values(by='token_holding', ascending=False).head(10)\n",
    "top10_sent = df_combined.sort_values(by='tokens_sent', ascending=False).head(10)[['address', 'tokens_sent']]\n",
    "top10_received = df_combined.sort_values(by='tokens_received', ascending=False).head(10)[['address', 'tokens_received']]\n",
    "\n",
    "# Totals for percentages\n",
    "total_token_holding = df_combined['token_holding'].sum()\n",
    "total_tokens_sent = df_combined['tokens_sent'].sum()\n",
    "total_tokens_received = df_combined['tokens_received'].sum()\n",
    "\n",
    "# Compute percentage share\n",
    "def compute_percentage(df, value_col, total_val):\n",
    "    df = df.copy()\n",
    "    df['percentage'] = (df[value_col] / total_val * 100).round(2)\n",
    "    return df\n",
    "\n",
    "summary_holdings = compute_percentage(top10_holdings[['address', 'token_holding']], 'token_holding', total_token_holding)\n",
    "summary_sent = compute_percentage(top10_sent, 'tokens_sent', total_tokens_sent)\n",
    "summary_received = compute_percentage(top10_received, 'tokens_received', total_tokens_received)\n",
    "\n",
    "# Merge all summaries\n",
    "summary_report = pd.merge(summary_holdings, summary_sent, on='address', how='outer')\n",
    "summary_report = pd.merge(summary_report, summary_received, on='address', how='outer')\n",
    "\n",
    "# Rename columns\n",
    "summary_report = summary_report.rename(columns={\n",
    "    'token_holding': 'Token Holding',\n",
    "    'percentage_x': '% of Total Holding',\n",
    "    'tokens_sent': 'Tokens Sent',\n",
    "    'percentage_y': '% of Total Sent',\n",
    "    'tokens_received': 'Tokens Received',\n",
    "    'percentage': '% of Total Received'\n",
    "})\n",
    "\n",
    "total_cleaned_records = df\n",
    "df = Total_cleaned_records\n",
    "\n",
    "\n",
    "#Task 4\n",
    "\n",
    "# ========= Task 4.1: Volume per Token per Day =========\n",
    "volume_per_day = df.groupby([df['timestamp'].dt.date, 'token.symbol'])['normalized_value'].sum().reset_index()\n",
    "volume_per_day.columns = ['date', 'token', 'daily_volume']\n",
    "\n",
    "# ========= Task 4.2: Cumulative Total Supply =========\n",
    "df_mint = df[df['type'] == 'token_minting'].copy()\n",
    "df_burn = df[df['type'] == 'token_burning'].copy()\n",
    "\n",
    "df_mint['minted'] = df_mint['normalized_value']\n",
    "df_burn['burned'] = df_burn['normalized_value']\n",
    "\n",
    "mint_burn = pd.concat([df_mint[['timestamp', 'token.symbol', 'minted']], \n",
    "                       df_burn[['timestamp', 'token.symbol', 'burned']]], sort=False).fillna(0)\n",
    "mint_burn['date'] = mint_burn['timestamp'].dt.date\n",
    "\n",
    "supply = mint_burn.groupby(['date', 'token.symbol']).agg({'minted': 'sum', 'burned': 'sum'}).reset_index()\n",
    "supply['net_minted'] = supply['minted'] - supply['burned']\n",
    "supply['cumulative_supply'] = supply.groupby('token.symbol')['net_minted'].cumsum()\n",
    "\n",
    "# ========= Task 4.3: Spike Detection =========\n",
    "spike_analysis = df.groupby([df['timestamp'].dt.date, 'token.symbol', 'type'])['normalized_value'].sum().reset_index()\n",
    "spike_analysis = spike_analysis.pivot(index=['timestamp', 'token.symbol'], columns='type', values='normalized_value').fillna(0)\n",
    "spike_analysis.reset_index(inplace=True)\n",
    "\n",
    "# ========= Task 4.4: Top Traded Tokens =========\n",
    "traded_volume = df[df['type'] == 'token_transfer'].groupby('token.symbol')['normalized_value'].sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "task4_volume_per_day = volume_per_day\n",
    "task4_cumulative_supply = supply\n",
    "task4_spike_analysis = spike_analysis\n",
    "task4_top_tokens = traded_volume\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "st.set_page_config(page_title=\"Token Tracker Dashboard\", layout=\"wide\")\n",
    "st.title(\"📡 Token Analytics from XCAP API\")\n",
    "\n",
    "with st.spinner(\"Fetching and processing data...\"):\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_summary = task3_summary_report\n",
    "    df_cleaned = Total_cleaned_records\n",
    "    df_trend = task4_volume_per_day   # parse_dates=['date']\n",
    "    df_supply = task4_cumulative_supply # parse_dates=['date'])\n",
    "    df_top = task4_top_tokens\n",
    "\n",
    "\n",
    "# the chats should use in memory cleaned data for 8 hours and refresh after 8 hours\n",
    "# dashboad creation\n",
    "\n",
    "st.set_page_config(page_title=\"Token Analytics Dashboard\", layout=\"wide\")\n",
    "st.title(\"📊 Token Distribution & Blockchain Trend Dashboard\")\n",
    "\n",
    "df_summary, df_cleaned, df_trend, df_supply, df_top = load_data()\n",
    "\n",
    "# === Sidebar Filters ===\n",
    "all_tokens = df_cleaned['token.symbol'].dropna().unique().tolist()\n",
    "all_types = df_cleaned['type'].dropna().unique().tolist()\n",
    "min_date = df_cleaned['timestamp'].min()\n",
    "max_date = df_cleaned['timestamp'].max()\n",
    "min_rate = float(df_cleaned['token.exchange_rate'].min())\n",
    "max_rate = float(df_cleaned['token.exchange_rate'].max())\n",
    "\n",
    "st.sidebar.header(\"🔍 Filter Transactions\")\n",
    "selected_token = st.sidebar.selectbox(\"Select Token Symbol\", [\"All\"] + all_tokens)\n",
    "selected_type = st.sidebar.selectbox(\"Select Transaction Type\", [\"All\"] + all_types)\n",
    "selected_date = st.sidebar.date_input(\"Filter by Date Range\", [min_date, max_date])\n",
    "rate_range = st.sidebar.slider(\"Token Exchange Rate\", min_value=0.001, max_value=1.0, value=(min_rate, max_rate))\n",
    "\n",
    "df_filtered = df_cleaned.copy()\n",
    "if selected_token != \"All\":\n",
    "    df_filtered = df_filtered[df_filtered['token.symbol'] == selected_token]\n",
    "if selected_type != \"All\":\n",
    "    df_filtered = df_filtered[df_filtered['type'] == selected_type]\n",
    "df_filtered = df_filtered[\n",
    "    (df_filtered['timestamp'].dt.date >= selected_date[0]) &\n",
    "    (df_filtered['timestamp'].dt.date <= selected_date[1]) &\n",
    "    (df_filtered['token.exchange_rate'] >= rate_range[0]) &\n",
    "    (df_filtered['token.exchange_rate'] <= rate_range[1])\n",
    "]\n",
    "\n",
    "# === Summary Table ===\n",
    "st.subheader(\"📦 Top Token Holders and Distribution Summary\")\n",
    "st.dataframe(df_summary.style.format({\n",
    "    \"Token Holding\": \"{:.6f}\",\n",
    "    \"% of Total Holding\": \"{:.2f}%\",\n",
    "    \"Tokens Sent\": \"{:.6f}\",\n",
    "    \"% of Total Sent\": \"{:.2f}%\",\n",
    "    \"Tokens Received\": \"{:.6f}\",\n",
    "    \"% of Total Received\": \"{:.2f}%\"\n",
    "}))\n",
    "\n",
    "# === Task 3 Charts ===\n",
    "st.subheader(\"📈 Visual Breakdown by Address\")\n",
    "col1, col2, col3 = st.columns(3)\n",
    "\n",
    "def labeled_barh(data, column, title, color):\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    bars = ax.barh(data['address'], data[column], color=color)\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.invert_yaxis()\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{bar.get_width():.2f}',\n",
    "                va='center', ha='left', fontsize=8)\n",
    "    return fig\n",
    "\n",
    "with col1:\n",
    "    st.pyplot(labeled_barh(df_summary, \"Token Holding\", \"Token Holdings\", \"green\"))\n",
    "\n",
    "with col2:\n",
    "    st.pyplot(labeled_barh(df_summary, \"Tokens Sent\", \"Tokens Sent\", \"red\"))\n",
    "\n",
    "with col3:\n",
    "    st.pyplot(labeled_barh(df_summary, \"Tokens Received\", \"Tokens Received\", \"blue\"))\n",
    "\n",
    "# === Task 4 Charts ===\n",
    "st.subheader(\"📉 Task 4: Blockchain Activity Trends\")\n",
    "col4, col5 = st.columns(2)\n",
    "\n",
    "with col4:\n",
    "    st.markdown(\"##### 🔄 Daily Token Transfer Volume\")\n",
    "    df_t4 = df_trend.copy()\n",
    "    if selected_token != \"All\":\n",
    "        df_t4 = df_t4[df_t4['token'] == selected_token]\n",
    "    fig4, ax4 = plt.subplots(figsize=(6, 3))\n",
    "    for token in df_t4['token'].unique():\n",
    "        token_df = df_t4[df_t4['token'] == token]\n",
    "        ax4.plot(token_df['date'], token_df['daily_volume'], marker='o', label=token)\n",
    "        for x, y in zip(token_df['date'], token_df['daily_volume']):\n",
    "            ax4.text(x, y, f\"{y:.2f}\", fontsize=7)\n",
    "    ax4.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax4.tick_params(axis='x', labelrotation=45, labelsize=8)\n",
    "    ax4.legend(fontsize=7)\n",
    "    st.pyplot(fig4)\n",
    "\n",
    "with col5:\n",
    "    st.markdown(\"##### 📈 Cumulative Token Supply\")\n",
    "    df_sup = df_supply.copy()\n",
    "    if selected_token != \"All\":\n",
    "        df_sup = df_sup[df_sup['token.symbol'] == selected_token]\n",
    "    fig5, ax5 = plt.subplots(figsize=(6, 3))\n",
    "    for token in df_sup['token.symbol'].unique():\n",
    "        token_df = df_sup[df_sup['token.symbol'] == token]\n",
    "        ax5.plot(token_df['date'], token_df['cumulative_supply'], marker='o', label=token)\n",
    "        for x, y in zip(token_df['date'], token_df['cumulative_supply']):\n",
    "            ax5.text(x, y, f\"{y:.2f}\", fontsize=7)\n",
    "    ax5.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax5.tick_params(axis='x', labelrotation=45, labelsize=8)\n",
    "    ax5.legend(fontsize=7)\n",
    "    st.pyplot(fig5)\n",
    "\n",
    "# === Top Tokens\n",
    "st.subheader(\"🏆 Most Transferred Tokens\")\n",
    "fig6, ax6 = plt.subplots(figsize=(6, 3))\n",
    "bars = ax6.barh(df_top['token.symbol'], df_top['total_transferred'], color='orange')\n",
    "ax6.set_xlabel(\"Total Transferred\")\n",
    "ax6.set_ylabel(\"Token\")\n",
    "ax6.tick_params(labelsize=8)\n",
    "ax6.invert_yaxis()\n",
    "for bar in bars:\n",
    "    ax6.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f\"{bar.get_width():.2f}\", fontsize=8, va='center', ha='left')\n",
    "st.pyplot(fig6)\n",
    "\n",
    "# === Raw Cleaned Table\n",
    "st.subheader(\"📄 Cleaned Token Transfer Records\")\n",
    "st.dataframe(df_filtered.reset_index(drop=True))\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"Made with ❤️ by Qenehelo Matjama \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
