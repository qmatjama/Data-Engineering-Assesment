{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7672d8-01d2-4fe0-8957-63c04be5cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import streamlit as st\n",
    "from functools import reduce\n",
    "\n",
    "# === Task 1: API Ingestion ===\n",
    "@st.cache_data(ttl=28800)  # cache for 8 hours\n",
    "def fetch_and_process_data():\n",
    "    base_url = \"https://xcap-mainnet.explorer.xcap.network/api/v2/token-transfers\"\n",
    "    MAX_PAGES = 50\n",
    "    PAGE_DELAY = 0.3\n",
    "\n",
    "    all_data = []\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        response = requests.get(base_url, params={\"page\": page}, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        items = response.json().get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_data.extend(items)\n",
    "        time.sleep(PAGE_DELAY)\n",
    "\n",
    "    df = pd.json_normalize(all_data)\n",
    "    fields_to_keep = [\n",
    "        'transaction_hash', 'token.symbol', 'total.value', 'from.hash', 'to.hash',\n",
    "        'timestamp', 'token.exchange_rate', 'type', 'token.decimals']\n",
    "    df = df[[col for col in fields_to_keep if col in df.columns]].drop_duplicates()\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "    df['total.value'] = pd.to_numeric(df['total.value'], errors='coerce').fillna(0)\n",
    "    df['token.decimals'] = pd.to_numeric(df['token.decimals'], errors='coerce').fillna(18)\n",
    "    df['normalized_value'] = df['total.value'] / (10 ** df['token.decimals'])\n",
    "    df['token.exchange_rate'] = pd.to_numeric(df['token.exchange_rate'], errors='coerce').fillna(0)\n",
    "    df['usd_value'] = df['normalized_value'] * df['token.exchange_rate']\n",
    "    return df\n",
    "\n",
    "# === Task 2-4 Processing ===\n",
    "def generate_metrics(df):\n",
    "    minted_total = df[df['type'] == 'token_minting']['normalized_value'].sum()\n",
    "    burned_total = df[df['type'] == 'token_burning']['normalized_value'].sum()\n",
    "    total_supply = minted_total - burned_total\n",
    "    unique_tokens = df['token.symbol'].nunique()\n",
    "    total_transactions = len(df)\n",
    "    transferred_tokens = df[df['type'] == 'token_transfer']['normalized_value'].sum()\n",
    "    total_usd_volume = df['usd_value'].sum()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Metric\": [\n",
    "            \"1. Total Asset Supply\", \"2. Unique Tokens\", \"3. Total Transactions\",\n",
    "            \"4. Tokens Minted\", \"5. Tokens Burned\", \"6. Tokens Transferred\",\n",
    "            \"7. Total Transaction Volume (USD)\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            total_supply, unique_tokens, total_transactions, minted_total,\n",
    "            burned_total, transferred_tokens, total_usd_volume\n",
    "        ]\n",
    "    })\n",
    "\n",
    "def summarize_top_holders(df):\n",
    "    tokens_sent = df[df['type'] == 'token_transfer'].groupby('from.hash')['normalized_value'].sum().reset_index()\n",
    "    tokens_sent.columns = ['address', 'tokens_sent']\n",
    "    tokens_received = df[df['type'] == 'token_transfer'].groupby('to.hash')['normalized_value'].sum().reset_index()\n",
    "    tokens_received.columns = ['address', 'tokens_received']\n",
    "    tokens_minted = df[df['type'] == 'token_minting'].groupby('to.hash')['normalized_value'].sum().reset_index()\n",
    "    tokens_minted.columns = ['address', 'tokens_minted']\n",
    "    tokens_burned = df[df['type'] == 'token_burning'].groupby('from.hash')['normalized_value'].sum().reset_index()\n",
    "    tokens_burned.columns = ['address', 'tokens_burned']\n",
    "\n",
    "    dfs = [tokens_minted, tokens_burned, tokens_received, tokens_sent]\n",
    "    df_combined = reduce(lambda l, r: pd.merge(l, r, on='address', how='outer'), dfs).fillna(0)\n",
    "    df_combined['token_holding'] = (\n",
    "        df_combined['tokens_minted'] - df_combined['tokens_burned'] +\n",
    "        df_combined['tokens_received'] - df_combined['tokens_sent']\n",
    "    )\n",
    "\n",
    "    return df_combined.sort_values(by='token_holding', ascending=False).head(10)\n",
    "\n",
    "def labeled_barh(data, column, title, color):\n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    bars = ax.barh(data['address'], data[column], color=color)\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.invert_yaxis()\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f\"{bar.get_width():.2f}\", va='center', ha='left', fontsize=8)\n",
    "    return fig\n",
    "\n",
    "# === Streamlit App ===\n",
    "st.set_page_config(page_title=\"Live Token Dashboard\", layout=\"wide\")\n",
    "st.title(\"📡 Real-Time Token Analytics from XCAP API\")\n",
    "\n",
    "with st.spinner(\"Loading data from XCAP API...\"):\n",
    "    df_cleaned = fetch_and_process_data()\n",
    "    metrics_df = generate_metrics(df_cleaned)\n",
    "    df_summary = summarize_top_holders(df_cleaned)\n",
    "\n",
    "# Filters\n",
    "min_date, max_date = df_cleaned['timestamp'].min(), df_cleaned['timestamp'].max()\n",
    "tokens = [\"All\"] + sorted(df_cleaned['token.symbol'].dropna().unique())\n",
    "types = [\"All\"] + sorted(df_cleaned['type'].dropna().unique())\n",
    "\n",
    "st.sidebar.header(\"🔍 Filters\")\n",
    "token = st.sidebar.selectbox(\"Token\", tokens)\n",
    "txn_type = st.sidebar.selectbox(\"Type\", types)\n",
    "date_range = st.sidebar.date_input(\"Date Range\", [min_date, max_date])\n",
    "rate_min, rate_max = st.sidebar.slider(\"Exchange Rate Range\", 0.001, 1.0, (0.001, 1.0))\n",
    "\n",
    "# Apply filters\n",
    "df_filtered = df_cleaned.copy()\n",
    "if token != \"All\":\n",
    "    df_filtered = df_filtered[df_filtered['token.symbol'] == token]\n",
    "if txn_type != \"All\":\n",
    "    df_filtered = df_filtered[df_filtered['type'] == txn_type]\n",
    "df_filtered = df_filtered[\n",
    "    (df_filtered['timestamp'].dt.date >= date_range[0]) &\n",
    "    (df_filtered['timestamp'].dt.date <= date_range[1]) &\n",
    "    (df_filtered['token.exchange_rate'] >= rate_min) &\n",
    "    (df_filtered['token.exchange_rate'] <= rate_max)\n",
    "]\n",
    "\n",
    "# Metrics & Summary\n",
    "df_summary = summarize_top_holders(df_filtered)\n",
    "st.subheader(\"📦 Key Blockchain Metrics\")\n",
    "st.dataframe(metrics_df)\n",
    "\n",
    "st.subheader(\"📊 Top Holders\")\n",
    "col1, col2, col3 = st.columns(3)\n",
    "with col1:\n",
    "    st.pyplot(labeled_barh(df_summary, \"token_holding\", \"Token Holdings\", \"green\"))\n",
    "with col2:\n",
    "    st.pyplot(labeled_barh(df_summary, \"tokens_sent\", \"Tokens Sent\", \"red\"))\n",
    "with col3:\n",
    "    st.pyplot(labeled_barh(df_summary, \"tokens_received\", \"Tokens Received\", \"blue\"))\n",
    "\n",
    "st.subheader(\"📄 Raw Data\")\n",
    "st.dataframe(df_filtered)\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"Made with ❤️ by Qenehelo | Live from the XCAP blockchain\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
