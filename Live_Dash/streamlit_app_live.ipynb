{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c42b76-eda5-46e6-8455-b9fec91cf577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from functools import reduce\n",
    "import streamlit as st\n",
    "\n",
    "# Streamlit config\n",
    "st.set_page_config(page_title=\"Token Tracker Dashboard\", layout=\"wide\")\n",
    "st.title(\"ðŸ“¡ Token Analytics from XCAP API\")\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://xcap-mainnet.explorer.xcap.network/api/v2/token-transfers\"\n",
    "MAX_PAGES = 50\n",
    "PAGE_DELAY = 0.3\n",
    "FIELDS_TO_KEEP = [\n",
    "    'transaction_hash',\n",
    "    'token.symbol',\n",
    "    'total.value',\n",
    "    'from.hash',\n",
    "    'to.hash',\n",
    "    'timestamp',\n",
    "    'token.exchange_rate',\n",
    "    'type',\n",
    "    'token.decimals'\n",
    "]\n",
    "\n",
    "@st.cache_data(ttl=28800)  # Cache for 8 hours\n",
    "def fetch_all_token_transfers():\n",
    "    page = 1\n",
    "    all_data = []\n",
    "\n",
    "    with st.spinner(\"Fetching data from XCAP API...\"):\n",
    "        while page <= MAX_PAGES:\n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params={\"page\": page}, timeout=10)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    st.warning(f\"Error {response.status_code} on page {page}\")\n",
    "                    break\n",
    "\n",
    "                data = response.json()\n",
    "                items = data.get(\"items\", [])\n",
    "\n",
    "                if not items:\n",
    "                    break\n",
    "\n",
    "                all_data.extend(items)\n",
    "                page += 1\n",
    "                time.sleep(PAGE_DELAY)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                st.error(f\"Request failed on page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "    df = pd.json_normalize(all_data)\n",
    "    available_fields = [col for col in FIELDS_TO_KEEP if col in df.columns]\n",
    "    return df[available_fields]\n",
    "\n",
    "def process_data(df):\n",
    "    # Clean and transform data\n",
    "    df = df.drop_duplicates(subset=FIELDS_TO_KEEP)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "    \n",
    "    # Convert numeric fields\n",
    "    df['total.value'] = pd.to_numeric(df['total.value'], errors='coerce').fillna(0)\n",
    "    df['token.decimals'] = pd.to_numeric(df['token.decimals'], errors='coerce').fillna(18)\n",
    "    df['normalized_value'] = df['total.value'] / (10 ** df['token.decimals'])\n",
    "    df['token.exchange_rate'] = pd.to_numeric(df['token.exchange_rate'], errors='coerce').fillna(0)\n",
    "    df['usd_value'] = df['normalized_value'] * df['token.exchange_rate']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    # Calculate key metrics\n",
    "    minted_total = df[df['type'] == 'token_minting']['normalized_value'].sum()\n",
    "    burned_total = df[df['type'] == 'token_burning']['normalized_value'].sum()\n",
    "    \n",
    "    metrics = {\n",
    "        \"Metric\": [\n",
    "            \"1. Total Asset Supply\",\n",
    "            \"2. Unique Tokens\",\n",
    "            \"3. Total Transactions\",\n",
    "            \"4. Tokens Minted\",\n",
    "            \"5. Tokens Burned\",\n",
    "            \"6. Tokens Transferred\",\n",
    "            \"7. Total Transaction Volume (USD)\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            minted_total - burned_total,\n",
    "            df['token.symbol'].nunique(),\n",
    "            len(df),\n",
    "            minted_total,\n",
    "            burned_total,\n",
    "            df[df['type'] == 'token_transfer']['normalized_value'].sum(),\n",
    "            df['usd_value'].sum()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def analyze_holdings(df):\n",
    "    # Token holdings analysis\n",
    "    tokens_sent = df[df['type'] == 'token_transfer'].groupby('from.hash')['normalized_value'].sum().reset_index()\n",
    "    tokens_sent.columns = ['address', 'tokens_sent']\n",
    "    \n",
    "    tokens_received = df[df['type'] == 'token_transfer'].groupby('to.hash')['normalized_value'].sum().reset_index()\n",
    "    tokens_received.columns = ['address', 'tokens_received']\n",
    "    \n",
    "    tokens_minted = df[df['type'] == 'token_minting'].groupby('to.hash')['normalized_value'].sum().reset_index()\n",
    "    tokens_minted.columns = ['address', 'tokens_minted']\n",
    "    \n",
    "    tokens_burned = df[df['type'] == 'token_burning'].groupby('from.hash')['normalized_value'].sum().reset_index()\n",
    "    tokens_burned.columns = ['address', 'tokens_burned']\n",
    "    \n",
    "    # Combine all data\n",
    "    dfs = [tokens_minted, tokens_burned, tokens_received, tokens_sent]\n",
    "    df_combined = reduce(lambda left, right: pd.merge(left, right, on='address', how='outer'), dfs).fillna(0)\n",
    "    df_combined['token_holding'] = (df_combined['tokens_minted'] - df_combined['tokens_burned']) + \\\n",
    "                                  (df_combined['tokens_received'] - df_combined['tokens_sent'])\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_token_holding = df_combined['token_holding'].sum()\n",
    "    total_tokens_sent = df_combined['tokens_sent'].sum()\n",
    "    total_tokens_received = df_combined['tokens_received'].sum()\n",
    "    \n",
    "    top10_holdings = df_combined.sort_values('token_holding', ascending=False).head(10)\n",
    "    top10_holdings['% of Total Holding'] = (top10_holdings['token_holding'] / total_token_holding * 100).round(2)\n",
    "    \n",
    "    top10_sent = df_combined.sort_values('tokens_sent', ascending=False).head(10)[['address', 'tokens_sent']]\n",
    "    top10_sent['% of Total Sent'] = (top10_sent['tokens_sent'] / total_tokens_sent * 100).round(2)\n",
    "    \n",
    "    top10_received = df_combined.sort_values('tokens_received', ascending=False).head(10)[['address', 'tokens_received']]\n",
    "    top10_received['% of Total Received'] = (top10_received['tokens_received'] / total_tokens_received * 100).round(2)\n",
    "    \n",
    "    # Merge summaries\n",
    "    summary_report = pd.merge(top10_holdings, top10_sent, on='address', how='outer')\n",
    "    summary_report = pd.merge(summary_report, top10_received, on='address', how='outer')\n",
    "    \n",
    "    return summary_report.rename(columns={\n",
    "        'token_holding': 'Token Holding',\n",
    "        'tokens_sent': 'Tokens Sent',\n",
    "        'tokens_received': 'Tokens Received'\n",
    "    })\n",
    "\n",
    "def analyze_trends(df):\n",
    "    # Daily volume\n",
    "    volume_per_day = df.groupby([df['timestamp'].dt.date, 'token.symbol'])['normalized_value'].sum().reset_index()\n",
    "    volume_per_day.columns = ['date', 'token', 'daily_volume']\n",
    "    \n",
    "    # Cumulative supply\n",
    "    df_mint = df[df['type'] == 'token_minting'].copy()\n",
    "    df_burn = df[df['type'] == 'token_burning'].copy()\n",
    "    \n",
    "    df_mint['minted'] = df_mint['normalized_value']\n",
    "    df_burn['burned'] = df_burn['normalized_value']\n",
    "    \n",
    "    mint_burn = pd.concat([df_mint[['timestamp', 'token.symbol', 'minted']], \n",
    "                         df_burn[['timestamp', 'token.symbol', 'burned']]], sort=False).fillna(0)\n",
    "    mint_burn['date'] = mint_burn['timestamp'].dt.date\n",
    "    \n",
    "    supply = mint_burn.groupby(['date', 'token.symbol']).agg({'minted': 'sum', 'burned': 'sum'}).reset_index()\n",
    "    supply['net_minted'] = supply['minted'] - supply['burned']\n",
    "    supply['cumulative_supply'] = supply.groupby('token.symbol')['net_minted'].cumsum()\n",
    "    \n",
    "    # Top traded tokens\n",
    "    traded_volume = df[df['type'] == 'token_transfer'].groupby('token.symbol')['normalized_value'].sum().reset_index()\n",
    "    traded_volume.columns = ['token.symbol', 'total_transferred']\n",
    "    \n",
    "    return volume_per_day, supply, traded_volume\n",
    "\n",
    "def create_plots(summary_report, volume_per_day, supply, top_tokens):\n",
    "    # Helper function for horizontal bar charts\n",
    "    def labeled_barh(data, column, title, color):\n",
    "        fig, ax = plt.subplots(figsize=(4, 3))\n",
    "        bars = ax.barh(data['address'], data[column], color=color)\n",
    "        ax.set_xlabel(column)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.tick_params(labelsize=8)\n",
    "        ax.invert_yaxis()\n",
    "        for bar in bars:\n",
    "            ax.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{bar.get_width():.2f}',\n",
    "                    va='center', ha='left', fontsize=8)\n",
    "        return fig\n",
    "    \n",
    "    # Create plots\n",
    "    fig1 = labeled_barh(summary_report, \"Token Holding\", \"Token Holdings\", \"green\")\n",
    "    fig2 = labeled_barh(summary_report, \"Tokens Sent\", \"Tokens Sent\", \"red\")\n",
    "    fig3 = labeled_barh(summary_report, \"Tokens Received\", \"Tokens Received\", \"blue\")\n",
    "    \n",
    "    # Daily volume plot\n",
    "    fig4, ax4 = plt.subplots(figsize=(6, 3))\n",
    "    for token in volume_per_day['token'].unique():\n",
    "        token_df = volume_per_day[volume_per_day['token'] == token]\n",
    "        ax4.plot(token_df['date'], token_df['daily_volume'], marker='o', label=token)\n",
    "        for x, y in zip(token_df['date'], token_df['daily_volume']):\n",
    "            ax4.text(x, y, f\"{y:.2f}\", fontsize=7)\n",
    "    ax4.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax4.tick_params(axis='x', labelrotation=45, labelsize=8)\n",
    "    ax4.legend(fontsize=7)\n",
    "    ax4.set_title(\"Daily Token Transfer Volume\")\n",
    "    \n",
    "    # Cumulative supply plot\n",
    "    fig5, ax5 = plt.subplots(figsize=(6, 3))\n",
    "    for token in supply['token.symbol'].unique():\n",
    "        token_df = supply[supply['token.symbol'] == token]\n",
    "        ax5.plot(token_df['date'], token_df['cumulative_supply'], marker='o', label=token)\n",
    "        for x, y in zip(token_df['date'], token_df['cumulative_supply']):\n",
    "            ax5.text(x, y, f\"{y:.2f}\", fontsize=7)\n",
    "    ax5.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax5.tick_params(axis='x', labelrotation=45, labelsize=8)\n",
    "    ax5.legend(fontsize=7)\n",
    "    ax5.set_title(\"Cumulative Token Supply\")\n",
    "    \n",
    "    # Top tokens plot\n",
    "    fig6, ax6 = plt.subplots(figsize=(6, 3))\n",
    "    bars = ax6.barh(top_tokens['token.symbol'], top_tokens['total_transferred'], color='orange')\n",
    "    ax6.set_xlabel(\"Total Transferred\")\n",
    "    ax6.set_ylabel(\"Token\")\n",
    "    ax6.tick_params(labelsize=8)\n",
    "    ax6.invert_yaxis()\n",
    "    for bar in bars:\n",
    "        ax6.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f\"{bar.get_width():.2f}\", \n",
    "                fontsize=8, va='center', ha='left')\n",
    "    ax6.set_title(\"Most Transferred Tokens\")\n",
    "    \n",
    "    return fig1, fig2, fig3, fig4, fig5, fig6\n",
    "\n",
    "def main():\n",
    "    # Load and process data\n",
    "    raw_data = fetch_all_token_transfers()\n",
    "    processed_data = process_data(raw_data)\n",
    "    metrics_df = calculate_metrics(processed_data)\n",
    "    summary_report = analyze_holdings(processed_data)\n",
    "    volume_per_day, supply, top_tokens = analyze_trends(processed_data)\n",
    "    \n",
    "    # Create plots\n",
    "    fig1, fig2, fig3, fig4, fig5, fig6 = create_plots(summary_report, volume_per_day, supply, top_tokens)\n",
    "    \n",
    "    # Dashboard layout\n",
    "    st.title(\"ðŸ“Š Token Distribution & Blockchain Trend Dashboard\")\n",
    "    \n",
    "    # Sidebar filters\n",
    "    st.sidebar.header(\"ðŸ” Filter Data\")\n",
    "    all_tokens = processed_data['token.symbol'].dropna().unique().tolist()\n",
    "    all_types = processed_data['type'].dropna().unique().tolist()\n",
    "    min_date = processed_data['timestamp'].min().date()\n",
    "    max_date = processed_data['timestamp'].max().date()\n",
    "    \n",
    "    selected_token = st.sidebar.selectbox(\"Select Token\", [\"All\"] + all_tokens)\n",
    "    selected_type = st.sidebar.selectbox(\"Select Transaction Type\", [\"All\"] + all_types)\n",
    "    date_range = st.sidebar.date_input(\"Select Date Range\", [min_date, max_date])\n",
    "    \n",
    "    # Filter data based on selection\n",
    "    df_filtered = processed_data.copy()\n",
    "    if selected_token != \"All\":\n",
    "        df_filtered = df_filtered[df_filtered['token.symbol'] == selected_token]\n",
    "    if selected_type != \"All\":\n",
    "        df_filtered = df_filtered[df_filtered['type'] == selected_type]\n",
    "    if len(date_range) == 2:\n",
    "        df_filtered = df_filtered[\n",
    "            (df_filtered['timestamp'].dt.date >= date_range[0]) & \n",
    "            (df_filtered['timestamp'].dt.date <= date_range[1])\n",
    "    \n",
    "    # Filter trend data\n",
    "    df_trend_filtered = volume_per_day.copy()\n",
    "    df_supply_filtered = supply.copy()\n",
    "    df_top_filtered = top_tokens.copy()\n",
    "    \n",
    "    if selected_token != \"All\":\n",
    "        df_trend_filtered = df_trend_filtered[df_trend_filtered['token'] == selected_token]\n",
    "        df_supply_filtered = df_supply_filtered[df_supply_filtered['token.symbol'] == selected_token]\n",
    "    \n",
    "    # Metrics section\n",
    "    st.subheader(\"ðŸ“Š Key Metrics\")\n",
    "    st.dataframe(metrics_df.style.format({\"Value\": \"{:,.2f}\"}))\n",
    "    \n",
    "    # Summary section\n",
    "    st.subheader(\"ðŸ“¦ Top Token Holders and Distribution Summary\")\n",
    "    st.dataframe(summary_report.style.format({\n",
    "        \"Token Holding\": \"{:.6f}\",\n",
    "        \"% of Total Holding\": \"{:.2f}%\",\n",
    "        \"Tokens Sent\": \"{:.6f}\",\n",
    "        \"% of Total Sent\": \"{:.2f}%\",\n",
    "        \"Tokens Received\": \"{:.6f}\",\n",
    "        \"% of Total Received\": \"{:.2f}%\"\n",
    "    }))\n",
    "    \n",
    "    # Holdings section\n",
    "    st.subheader(\"ðŸ“ˆ Visual Breakdown by Address\")\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    with col1:\n",
    "        st.pyplot(fig1)\n",
    "    with col2:\n",
    "        st.pyplot(fig2)\n",
    "    with col3:\n",
    "        st.pyplot(fig3)\n",
    "    \n",
    "    # Trends section\n",
    "    st.subheader(\"ðŸ“‰ Blockchain Activity Trends\")\n",
    "    col4, col5 = st.columns(2)\n",
    "    with col4:\n",
    "        st.markdown(\"##### ðŸ”„ Daily Token Transfer Volume\")\n",
    "        st.pyplot(fig4)\n",
    "    with col5:\n",
    "        st.markdown(\"##### ðŸ“ˆ Cumulative Token Supply\")\n",
    "        st.pyplot(fig5)\n",
    "    \n",
    "    # Top tokens section\n",
    "    st.subheader(\"ðŸ† Most Transferred Tokens\")\n",
    "    st.pyplot(fig6)\n",
    "    \n",
    "    # Raw data section\n",
    "    st.subheader(\"ðŸ“„ Cleaned Token Transfer Records\")\n",
    "    st.dataframe(df_filtered.reset_index(drop=True))\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"Made with â¤ï¸ by Qenehelo Matjama\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
